{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{"id":"zzCb6lOnJNpK"}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport imageio","metadata":{"id":"U2A6hg1CJNpL","execution":{"iopub.status.busy":"2022-12-17T07:40:19.805958Z","iopub.execute_input":"2022-12-17T07:40:19.806381Z","iopub.status.idle":"2022-12-17T07:40:31.367132Z","shell.execute_reply.started":"2022-12-17T07:40:19.806293Z","shell.execute_reply":"2022-12-17T07:40:31.366084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set hyperparameters","metadata":{"id":"UNM36XOKJNpM"}},{"cell_type":"code","source":"batch_size = 64\nnum_channels = 1\nnum_classes = 10\nimage_size = 28\nlatent_dim = 128","metadata":{"id":"OqcqBBciJNpM","execution":{"iopub.status.busy":"2022-12-17T07:40:31.369045Z","iopub.execute_input":"2022-12-17T07:40:31.369733Z","iopub.status.idle":"2022-12-17T07:40:31.378656Z","shell.execute_reply.started":"2022-12-17T07:40:31.369695Z","shell.execute_reply":"2022-12-17T07:40:31.377724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the MNIST dataset and preprocessing it","metadata":{"id":"vYnRTBFDJNpO"}},{"cell_type":"code","source":"# We'll use all the available examples from both the training and test\n# sets.\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nall_digits = np.concatenate([x_train, x_test])\nall_labels = np.concatenate([y_train, y_test])\n\n# Scale the pixel values to [0, 1] range, add a channel dimension to\n# the images, and one-hot encode the labels.\nall_digits = all_digits.astype(\"float32\") / 255.0\nall_digits = np.reshape(all_digits, (-1, 28, 28, 1))\nall_labels = keras.utils.to_categorical(all_labels, 10)\n\n# Create tf.data.Dataset.\ndataset = tf.data.Dataset.from_tensor_slices((all_digits, all_labels))\ndataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n\nprint(f\"Shape of training images: {all_digits.shape}\")\nprint(f\"Shape of training labels: {all_labels.shape}\")","metadata":{"id":"KU9tyGuNJNpP","outputId":"8d4c7d3a-cd00-43f0-fb53-718a41b9db17","execution":{"iopub.status.busy":"2022-12-17T07:40:31.379993Z","iopub.execute_input":"2022-12-17T07:40:31.380271Z","iopub.status.idle":"2022-12-17T07:40:37.883464Z","shell.execute_reply.started":"2022-12-17T07:40:31.380246Z","shell.execute_reply":"2022-12-17T07:40:37.882343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculating the number of input channel for the generator and discriminator\n\nIn Condiational GAN, class labeles need to be taken into consideration. Thus, the number of classes is added to\nthe input channels of the generator (noise vector) as well as the discriminator\n(generated image input).","metadata":{"id":"YHXzcEQ3JNpQ"}},{"cell_type":"code","source":"generator_in_channels = latent_dim + num_classes\ndiscriminator_in_channels = num_channels + num_classes\nprint(generator_in_channels, discriminator_in_channels)","metadata":{"id":"j0pPuMdMJNpR","outputId":"11197381-b061-4237-95d2-7a3d1de42bc8","execution":{"iopub.status.busy":"2022-12-17T07:40:37.886239Z","iopub.execute_input":"2022-12-17T07:40:37.886974Z","iopub.status.idle":"2022-12-17T07:40:37.893318Z","shell.execute_reply.started":"2022-12-17T07:40:37.886931Z","shell.execute_reply":"2022-12-17T07:40:37.892287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build the discriminator and generator\n","metadata":{"id":"VRL4br1rJNpS"}},{"cell_type":"code","source":"# Create the discriminator.\ndiscriminator = keras.Sequential(\n    [\n        keras.layers.InputLayer((28, 28, discriminator_in_channels)),\n        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.GlobalMaxPooling2D(),\n        layers.Dense(1),\n    ],\n    name=\"discriminator\",\n)\n\n# Create the generator.\ngenerator = keras.Sequential(\n    [\n        keras.layers.InputLayer((generator_in_channels,)),\n        # We want to generate 128 + num_classes coefficients to reshape into a\n        # 7x7x(128 + num_classes) map.\n        layers.Dense(7 * 7 * generator_in_channels),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Reshape((7, 7, generator_in_channels)),\n        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n        layers.LeakyReLU(alpha=0.2),\n        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n    ],\n    name=\"generator\",\n)","metadata":{"id":"ibKDCHJqJNpS","execution":{"iopub.status.busy":"2022-12-17T07:40:37.895219Z","iopub.execute_input":"2022-12-17T07:40:37.895646Z","iopub.status.idle":"2022-12-17T07:40:38.195259Z","shell.execute_reply.started":"2022-12-17T07:40:37.895610Z","shell.execute_reply":"2022-12-17T07:40:38.194299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build a `ConditionalGAN` model","metadata":{"id":"gyGXHt_EJNpT"}},{"cell_type":"code","source":"class ConditionalGAN(keras.Model):\n    def __init__(self, discriminator, generator, latent_dim):\n        super(ConditionalGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n\n    @property\n    def metrics(self):\n        return [self.gen_loss_tracker, self.disc_loss_tracker]\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super(ConditionalGAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n\n    def train_step(self, data):\n        # Unpack the data.\n        real_images, one_hot_labels = data\n\n        # Add dummy dimensions to the labels so that they can be concatenated with\n        # the images. This is for the discriminator.\n        image_one_hot_labels = one_hot_labels[:, :, None, None]\n        image_one_hot_labels = tf.repeat(\n            image_one_hot_labels, repeats=[image_size * image_size]\n        )\n        image_one_hot_labels = tf.reshape(\n            image_one_hot_labels, (-1, image_size, image_size, num_classes)\n        )\n\n        # Sample random points in the latent space and concatenate the labels.\n        # This is for the generator.\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n        random_vector_labels = tf.concat(\n            [random_latent_vectors, one_hot_labels], axis=1\n        )\n\n        # Decode the noise (guided by labels) to fake images.\n        generated_images = self.generator(random_vector_labels)\n\n        # Combine them with real images. Note that we are concatenating the labels\n        # with these images here.\n        fake_image_and_labels = tf.concat([generated_images, image_one_hot_labels], -1)\n        real_image_and_labels = tf.concat([real_images, image_one_hot_labels], -1)\n        combined_images = tf.concat(\n            [fake_image_and_labels, real_image_and_labels], axis=0\n        )\n\n        # Assemble labels discriminating real from fake images.\n        labels = tf.concat(\n            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n        )\n\n        # Train the discriminator.\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(\n            zip(grads, self.discriminator.trainable_weights)\n        )\n\n        # Sample random points in the latent space.\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n        random_vector_labels = tf.concat(\n            [random_latent_vectors, one_hot_labels], axis=1\n        )\n\n        # Assemble labels that say \"all real images\".\n        misleading_labels = tf.zeros((batch_size, 1))\n\n        # Train the generator (note that we should *not* update the weights\n        # of the discriminator)!\n        with tf.GradientTape() as tape:\n            fake_images = self.generator(random_vector_labels)\n            fake_image_and_labels = tf.concat([fake_images, image_one_hot_labels], -1)\n            predictions = self.discriminator(fake_image_and_labels)\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n\n        # Monitor loss.\n        self.gen_loss_tracker.update_state(g_loss)\n        self.disc_loss_tracker.update_state(d_loss)\n        return {\n            \"g_loss\": self.gen_loss_tracker.result(),\n            \"d_loss\": self.disc_loss_tracker.result(),\n        }\n","metadata":{"id":"5mI-gG8NJNpU","execution":{"iopub.status.busy":"2022-12-17T07:40:38.196666Z","iopub.execute_input":"2022-12-17T07:40:38.196987Z","iopub.status.idle":"2022-12-17T07:40:38.213738Z","shell.execute_reply.started":"2022-12-17T07:40:38.196951Z","shell.execute_reply":"2022-12-17T07:40:38.212822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Start Training of Conditional GAN","metadata":{"id":"WegVXc3cJNpV","execution":{"iopub.status.busy":"2022-12-17T07:40:38.215144Z","iopub.execute_input":"2022-12-17T07:40:38.215632Z","iopub.status.idle":"2022-12-17T07:40:38.235384Z","shell.execute_reply.started":"2022-12-17T07:40:38.215587Z","shell.execute_reply":"2022-12-17T07:40:38.234456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cond_gan = ConditionalGAN(\n    discriminator=discriminator, generator=generator, latent_dim=latent_dim\n)\ncond_gan.compile(\n    d_optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n    g_optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n)\n\ncond_gan.fit(dataset, epochs=20)","metadata":{"id":"CXNeIfROJNpV","outputId":"efdc373f-5057-4a04-9540-5034e2c7dcb3","execution":{"iopub.status.busy":"2022-12-17T07:40:38.237211Z","iopub.execute_input":"2022-12-17T07:40:38.237609Z","iopub.status.idle":"2022-12-17T07:52:57.218345Z","shell.execute_reply.started":"2022-12-17T07:40:38.237571Z","shell.execute_reply":"2022-12-17T07:52:57.217355Z"},"trusted":true},"execution_count":null,"outputs":[]}]}