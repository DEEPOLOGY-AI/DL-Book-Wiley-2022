{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e8bdbfe4-1ca6-4c99-94f3-1ffba55c72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5debc1-1d48-4e1d-b3ba-7c6adaf05356",
   "metadata": {},
   "source": [
    "# 4.5.1 Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1c98ab-7b6a-462d-bddc-3aaf891f8e67",
   "metadata": {},
   "source": [
    "## 4.5.1.1 Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7caff2e0-7050-4d8d-ab7f-d4ab53c66fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(y_true, predictions):\n",
    "    y_true, predictions = np.array(y_true), np.array(predictions)\n",
    "    return np.mean(np.abs(y_true - predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "089a23e5-6265-441a-86a5-9d64785b48ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "true = [1,2,3,4,5,6]\n",
    "predicted = [1,3,4,4,5,9]\n",
    "\n",
    "print(MAE(true, predicted))\n",
    "print(mean_absolute_error(true, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218b586-8e30-4d39-8973-103cb7756afc",
   "metadata": {},
   "source": [
    "## 4.5.1.2 Root Mean Squared Error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32c70a70-8e23-4ea5-8405-5f52f59ca55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(actual, predicted):\n",
    "    actual = np.array(actual)\n",
    "    predicted = np.array(predicted)\n",
    "    differences = np.subtract(actual, predicted)\n",
    "    squared_differences = np.square(differences)\n",
    "    return math.sqrt(squared_differences.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d72b451f-e58d-4fe7-a614-9d7b88024e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35400640077266\n",
      "1.35400640077266\n"
     ]
    }
   ],
   "source": [
    "print(RMSE(true, predicted))\n",
    "print(math.sqrt(mean_squared_error(true, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a55f707-5f19-4783-9b9a-31e7af82b54d",
   "metadata": {},
   "source": [
    "## 4.5.1.3 Coefficient of Determination (R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64df7b69-8d16-43e4-89d9-9d9970cd18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_square(y_true, y_hat):\n",
    "    y_bar = np.mean(y_true)\n",
    "    se_total = np.sum((y_true - y_bar) ** 2)\n",
    "    se_explained = np.sum((y_hat - y_bar) ** 2)\n",
    "    se_residual = np.sum(np.subtract(y_true,y_hat) ** 2)\n",
    "    r2=1 - (se_residual / se_total)\n",
    "    \n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f5619de-156b-49fb-a8bb-e5cebb90270d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented R-squared = 0.37142857142857144\n"
     ]
    }
   ],
   "source": [
    "print(\"Implemented R-squared =\", R_square(true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d1c3180-5530-4a57-bd98-74c29ba038cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-Learn's R-squared =  0.37142857142857144\n"
     ]
    }
   ],
   "source": [
    "print(\"Scikit-Learn's R-squared = \",r2_score(true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be9db4f0-0501-4788-8572-0572b90b174e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2142857142857143"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=R_square(true, predicted)\n",
    "adj_r2 = 1 - ((1 - r2) * (len(true) - 1) / (len(true) - 1 - 1))\n",
    "adj_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff943ee-de40-433a-a853-fa686790f64f",
   "metadata": {},
   "source": [
    "# 4.5.2 Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f030398-5362-40d9-ab71-ee20530da083",
   "metadata": {},
   "source": [
    "## 4.5.2.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "874ed666-1f25-4eca-9a13-7feeb5d305b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(actual, predicted):\n",
    "    unique = set(actual)\n",
    "    matrix = [list() for x in range(len(unique))]\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [0 for x in range(len(unique))]\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for i in range(len(actual)):\n",
    "        x = lookup[actual[i]]\n",
    "        y = lookup[predicted[i]]\n",
    "        matrix[y][x] += 1\n",
    "    return unique, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "213ba1bb-d934-4a63-8bd9-8578b25e653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(unique, matrix):\n",
    "    print('(A)' + ' '.join(str(x) for x in unique))\n",
    "    print('(P)---')\n",
    "    for i, x in enumerate(unique):\n",
    "        print(\"%s| %s\" % (x, ' '.join(str(x) for x in matrix[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a6a083f-1df0-4076-92d9-abeda3d1a6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A)0 1\n",
      "(P)---\n",
      "0| 3 1\n",
      "1| 2 4\n"
     ]
    }
   ],
   "source": [
    "print_confusion_matrix(unique, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6797b15d-c717-4031-b737-7fa4a1a4c458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n",
      "[[3, 1], [2, 4]]\n"
     ]
    }
   ],
   "source": [
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,1,0,0,1,0,1,1,1]\n",
    "unique, matrix = confusion_matrix(actual, predicted)\n",
    "print(unique)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4cf0a-3ba0-4fef-8834-c2c42ee1a2ec",
   "metadata": {},
   "source": [
    "## 4.5.2.2 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "230a6436-84cb-4317-b2d3-6793f3263a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage between two lists\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4c70d15-0dd7-4155-af57-906c037dbaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
    "accuracy = accuracy_metric(actual, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c45293-d75d-4330-b94a-6539cee6e8aa",
   "metadata": {},
   "source": [
    "## 4.5.2.3 Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9036ff81-e52f-416c-ac07-d20fbdf5bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the Precision metric (Some times called Specificity): #(True Positives / float(True Positives + False Posivites))  \n",
    "def macro_precision(y_test , y_pred):\n",
    "    PrecisionScore = precision_score(y_test, y_pred , average='macro') \n",
    "    return PrecisionScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f66b2f45-090d-440b-b57d-0ba477b09157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = macro_precision(actual, predicted)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264acc5b-3f7a-4754-b9ba-21e6b5932e7a",
   "metadata": {},
   "source": [
    "## 4.5.2.4 Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "41b8aa92-9ded-4bf8-bf61-cd5a6d90fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the Recall Score : (Some times called Sensitivity) (TP / float(TP + FN))   1 / 1+2  \n",
    "def macro_recall(y_test , y_pred):\n",
    "    RecallScore = recall_score(y_test, y_pred , average='macro') \n",
    "    return  RecallScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a479f4c-b59d-4afb-bc48-6aeba4f3e711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = macro_recall(actual, predicted)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f42957-87f8-4ff5-a983-10dece81ce75",
   "metadata": {},
   "source": [
    "## 4.5.2.6 F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9f05a180-2bb6-49d6-b095-1c74c92ff5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the F1 Score  : 2 * (precision * recall) / (precision + recall)\n",
    "def macro_F1Score(y_test , y_pred):\n",
    "    F1Score = f1_score(y_test, y_pred , average='macro') \n",
    "    return F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "06b0426d-ede4-4c6d-8c29-6ed99e51ac56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8000000000000002"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1Score = macro_F1Score(actual, predicted)\n",
    "F1Score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
