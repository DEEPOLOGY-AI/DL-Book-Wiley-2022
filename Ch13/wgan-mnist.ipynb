{"metadata":{"colab":{"name":"wgan_gp","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n","metadata":{"id":"rJbNo45FviuA","execution":{"iopub.status.busy":"2022-12-17T08:34:23.047545Z","iopub.execute_input":"2022-12-17T08:34:23.048070Z","iopub.status.idle":"2022-12-17T08:34:29.469545Z","shell.execute_reply.started":"2022-12-17T08:34:23.047964Z","shell.execute_reply":"2022-12-17T08:34:29.468504Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"IMG_SHAPE = (28, 28, 1)\nBATCH_SIZE = 512\n\n# Size of the noise vector\nnoise_dim = 128\n\nmnist = keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\nprint(f\"Number of examples: {len(train_images)}\")\nprint(f\"Shape of the images in the dataset: {train_images.shape[1:]}\")\n\n# Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range\ntrain_images = train_images.reshape(train_images.shape[0], *IMG_SHAPE).astype(\"float32\")\ntrain_images = (train_images - 127.5) / 127.5","metadata":{"id":"R8SyPhahviuE","outputId":"eecfa695-5ff7-401d-ca2c-ae5b217a49d7","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2022-12-17T08:34:29.471602Z","iopub.execute_input":"2022-12-17T08:34:29.472245Z","iopub.status.idle":"2022-12-17T08:34:30.102269Z","shell.execute_reply.started":"2022-12-17T08:34:29.472206Z","shell.execute_reply":"2022-12-17T08:34:30.101053Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n11501568/11490434 [==============================] - 0s 0us/step\nNumber of examples: 60000\nShape of the images in the dataset: (28, 28)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Build the discriminator (the critic in the original WGAN)","metadata":{"id":"Tdi--3rjviuG"}},{"cell_type":"code","source":"def conv_block(\n    x,\n    filters,\n    activation,\n    kernel_size=(3, 3),\n    strides=(1, 1),\n    padding=\"same\",\n    use_bias=True,\n    use_bn=False,\n    use_dropout=False,\n    drop_value=0.5,\n):\n    x = layers.Conv2D(\n        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n    )(x)\n    if use_bn:\n        x = layers.BatchNormalization()(x)\n    x = activation(x)\n    if use_dropout:\n        x = layers.Dropout(drop_value)(x)\n    return x\n\n\ndef get_discriminator_model():\n    img_input = layers.Input(shape=IMG_SHAPE)\n    # Zero pad the input to make the input images size to (32, 32, 1).\n    x = layers.ZeroPadding2D((2, 2))(img_input)\n    x = conv_block(\n        x,\n        64,\n        kernel_size=(5, 5),\n        strides=(2, 2),\n        use_bn=False,\n        use_bias=True,\n        activation=layers.LeakyReLU(0.2),\n        use_dropout=False,\n        drop_value=0.3,\n    )\n    x = conv_block(\n        x,\n        128,\n        kernel_size=(5, 5),\n        strides=(2, 2),\n        use_bn=False,\n        activation=layers.LeakyReLU(0.2),\n        use_bias=True,\n        use_dropout=True,\n        drop_value=0.3,\n    )\n    x = conv_block(\n        x,\n        256,\n        kernel_size=(5, 5),\n        strides=(2, 2),\n        use_bn=False,\n        activation=layers.LeakyReLU(0.2),\n        use_bias=True,\n        use_dropout=True,\n        drop_value=0.3,\n    )\n    x = conv_block(\n        x,\n        512,\n        kernel_size=(5, 5),\n        strides=(2, 2),\n        use_bn=False,\n        activation=layers.LeakyReLU(0.2),\n        use_bias=True,\n        use_dropout=False,\n        drop_value=0.3,\n    )\n\n    x = layers.Flatten()(x)\n    x = layers.Dropout(0.2)(x)\n    x = layers.Dense(1)(x)\n\n    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n    return d_model\n\n\nd_model = get_discriminator_model()\nd_model.summary()","metadata":{"id":"XNElECmOviuI","outputId":"f206b94f-252e-40b9-bba2-ba6024eff849","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2022-12-17T08:34:30.105467Z","iopub.execute_input":"2022-12-17T08:34:30.105784Z","iopub.status.idle":"2022-12-17T08:34:34.166943Z","shell.execute_reply.started":"2022-12-17T08:34:30.105755Z","shell.execute_reply":"2022-12-17T08:34:34.165893Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2022-12-17 08:34:30.276050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.277012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.398596: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.399461: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.400206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.400948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.402433: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-17 08:34:30.666244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.667148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.667952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.668768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.669562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:30.670328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:33.611206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:33.612106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:33.612946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:33.613798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:33.614519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:33.615181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2022-12-17 08:34:33.618831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-17 08:34:33.619568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Model: \"discriminator\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nzero_padding2d (ZeroPadding2 (None, 32, 32, 1)         0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 16, 16, 64)        1664      \n_________________________________________________________________\nleaky_re_lu (LeakyReLU)      (None, 16, 16, 64)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 8, 8, 128)         204928    \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 128)         0         \n_________________________________________________________________\ndropout (Dropout)            (None, 8, 8, 128)         0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 4, 4, 256)         819456    \n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 256)         0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 4, 4, 256)         0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 2, 2, 512)         3277312   \n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 2, 2, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2048)              0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 2048)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 2049      \n=================================================================\nTotal params: 4,305,409\nTrainable params: 4,305,409\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create the generator","metadata":{"id":"GE9NKCcVviuK"}},{"cell_type":"code","source":"\ndef upsample_block(\n    x,\n    filters,\n    activation,\n    kernel_size=(3, 3),\n    strides=(1, 1),\n    up_size=(2, 2),\n    padding=\"same\",\n    use_bn=False,\n    use_bias=True,\n    use_dropout=False,\n    drop_value=0.3,\n):\n    x = layers.UpSampling2D(up_size)(x)\n    x = layers.Conv2D(\n        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n    )(x)\n\n    if use_bn:\n        x = layers.BatchNormalization()(x)\n\n    if activation:\n        x = activation(x)\n    if use_dropout:\n        x = layers.Dropout(drop_value)(x)\n    return x\n\n\ndef get_generator_model():\n    noise = layers.Input(shape=(noise_dim,))\n    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n\n    x = layers.Reshape((4, 4, 256))(x)\n    x = upsample_block(\n        x,\n        128,\n        layers.LeakyReLU(0.2),\n        strides=(1, 1),\n        use_bias=False,\n        use_bn=True,\n        padding=\"same\",\n        use_dropout=False,\n    )\n    x = upsample_block(\n        x,\n        64,\n        layers.LeakyReLU(0.2),\n        strides=(1, 1),\n        use_bias=False,\n        use_bn=True,\n        padding=\"same\",\n        use_dropout=False,\n    )\n    x = upsample_block(\n        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n    )\n    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n    # We will use a Cropping2D layer to make it (28, 28, 1).\n    x = layers.Cropping2D((2, 2))(x)\n\n    g_model = keras.models.Model(noise, x, name=\"generator\")\n    return g_model\n\n\ng_model = get_generator_model()\ng_model.summary()","metadata":{"id":"lubMYvTyviuL","outputId":"8ab3fce2-ee99-4fcc-ac10-39e9a7dee0b4","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2022-12-17T08:34:34.169709Z","iopub.execute_input":"2022-12-17T08:34:34.170230Z","iopub.status.idle":"2022-12-17T08:34:34.289174Z","shell.execute_reply.started":"2022-12-17T08:34:34.170192Z","shell.execute_reply":"2022-12-17T08:34:34.288321Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Model: \"generator\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 128)]             0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 4096)              524288    \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 4096)              16384     \n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 4096)              0         \n_________________________________________________________________\nreshape (Reshape)            (None, 4, 4, 256)         0         \n_________________________________________________________________\nup_sampling2d (UpSampling2D) (None, 8, 8, 256)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 8, 8, 128)         294912    \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 8, 8, 128)         512       \n_________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 128)         0         \n_________________________________________________________________\nup_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 16, 16, 64)        73728     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 16, 16, 64)        256       \n_________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 64)        0         \n_________________________________________________________________\nup_sampling2d_2 (UpSampling2 (None, 32, 32, 64)        0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 32, 32, 1)         576       \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 32, 32, 1)         4         \n_________________________________________________________________\nactivation (Activation)      (None, 32, 32, 1)         0         \n_________________________________________________________________\ncropping2d (Cropping2D)      (None, 28, 28, 1)         0         \n=================================================================\nTotal params: 910,660\nTrainable params: 902,082\nNon-trainable params: 8,578\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Create the WGAN-GP model\n\nNow that we have defined our generator and discriminator, it's time to implement\nthe WGAN-GP model. We will also override the `train_step` for training.","metadata":{"id":"sBG7xQtaviuN"}},{"cell_type":"code","source":"class WGAN(keras.Model):\n    def __init__(\n        self,\n        discriminator,\n        generator,\n        latent_dim,\n        discriminator_extra_steps=3,\n        gp_weight=10.0,\n    ):\n        super(WGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.latent_dim = latent_dim\n        self.d_steps = discriminator_extra_steps\n        self.gp_weight = gp_weight\n\n    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n        super(WGAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_fn = d_loss_fn\n        self.g_loss_fn = g_loss_fn\n\n    def gradient_penalty(self, batch_size, real_images, fake_images):\n        \"\"\"Calculates the gradient penalty.\n\n        \"\"\"\n        # Get the interpolated image\n        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n        diff = fake_images - real_images\n        interpolated = real_images + alpha * diff\n\n        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            # 1. Get the discriminator output for this interpolated image.\n            pred = self.discriminator(interpolated, training=True)\n\n        # 2. Calculate the gradients w.r.t to this interpolated image.\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        # 3. Calculate the norm of the gradients.\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        gp = tf.reduce_mean((norm - 1.0) ** 2)\n        return gp\n\n    def train_step(self, real_images):\n        if isinstance(real_images, tuple):\n            real_images = real_images[0]\n\n        # Get the batch size\n        batch_size = tf.shape(real_images)[0]\n\n        for i in range(self.d_steps):\n            # Get the latent vector\n            random_latent_vectors = tf.random.normal(\n                shape=(batch_size, self.latent_dim)\n            )\n            with tf.GradientTape() as tape:\n                \n                fake_images = self.generator(random_latent_vectors, training=True)\n                \n                fake_logits = self.discriminator(fake_images, training=True)\n\n                real_logits = self.discriminator(real_images, training=True)\n                \n                \n                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n                \n                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n                \n                d_loss = d_cost + gp * self.gp_weight\n\n            \n            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n            \n            self.d_optimizer.apply_gradients(\n                zip(d_gradient, self.discriminator.trainable_variables)\n            )\n\n        # Train the generator\n        # Get the latent vector\n        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n        with tf.GradientTape() as tape:\n            # Generate fake images using the generator\n            generated_images = self.generator(random_latent_vectors, training=True)\n            # Get the discriminator logits for fake images\n            gen_img_logits = self.discriminator(generated_images, training=True)\n            # Calculate the generator loss\n            g_loss = self.g_loss_fn(gen_img_logits)\n\n        # Get the gradients w.r.t the generator loss\n        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n        # Update the weights of the generator using the generator optimizer\n        self.g_optimizer.apply_gradients(\n            zip(gen_gradient, self.generator.trainable_variables)\n        )\n        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n","metadata":{"id":"nK6uhR6oviuN","execution":{"iopub.status.busy":"2022-12-17T08:34:34.290709Z","iopub.execute_input":"2022-12-17T08:34:34.291031Z","iopub.status.idle":"2022-12-17T08:34:34.309623Z","shell.execute_reply.started":"2022-12-17T08:34:34.290998Z","shell.execute_reply":"2022-12-17T08:34:34.308757Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Create a Keras callback that periodically saves generated images","metadata":{"id":"i1jcm27GviuP"}},{"cell_type":"code","source":"class GANMonitor(keras.callbacks.Callback):\n    def __init__(self, num_img=6, latent_dim=128):\n        self.num_img = num_img\n        self.latent_dim = latent_dim\n\n    def on_epoch_end(self, epoch, logs=None):\n        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n        generated_images = self.model.generator(random_latent_vectors)\n        generated_images = (generated_images * 127.5) + 127.5\n\n        for i in range(self.num_img):\n            img = generated_images[i].numpy()\n            img = keras.preprocessing.image.array_to_img(img)\n            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))\n","metadata":{"id":"dq_6HoXNviuQ","execution":{"iopub.status.busy":"2022-12-17T08:34:34.311022Z","iopub.execute_input":"2022-12-17T08:34:34.312495Z","iopub.status.idle":"2022-12-17T08:34:34.323765Z","shell.execute_reply.started":"2022-12-17T08:34:34.312461Z","shell.execute_reply":"2022-12-17T08:34:34.322767Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Train the end-to-end model","metadata":{"id":"o7sfCqY0viuR"}},{"cell_type":"code","source":"generator_optimizer = keras.optimizers.Adam(\n    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n)\ndiscriminator_optimizer = keras.optimizers.Adam(\n    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n)\n\n\ndef discriminator_loss(real_img, fake_img):\n    real_loss = tf.reduce_mean(real_img)\n    fake_loss = tf.reduce_mean(fake_img)\n    return fake_loss - real_loss\n\n\ndef generator_loss(fake_img):\n    return -tf.reduce_mean(fake_img)\n\n\nepochs = 20\n\ncbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n\nwgan = WGAN(\n    discriminator=d_model,\n    generator=g_model,\n    latent_dim=noise_dim,\n    discriminator_extra_steps=3,\n)\n\n# Compile the wgan model\nwgan.compile(\n    d_optimizer=discriminator_optimizer,\n    g_optimizer=generator_optimizer,\n    g_loss_fn=generator_loss,\n    d_loss_fn=discriminator_loss,\n)\n\n# Start training\nwgan.fit(train_images, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])","metadata":{"id":"kZlq1m3jviuS","outputId":"94a13e9b-4d07-41dd-e541-5cf2aaf8b648","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2022-12-17T08:34:34.325421Z","iopub.execute_input":"2022-12-17T08:34:34.325785Z","iopub.status.idle":"2022-12-17T09:19:15.382203Z","shell.execute_reply.started":"2022-12-17T08:34:34.325751Z","shell.execute_reply":"2022-12-17T09:19:15.381020Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2022-12-17 08:34:34.762426: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"2022-12-17 08:34:42.635937: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"118/118 [==============================] - 143s 1s/step - d_loss: -16.3115 - g_loss: -18.0137\nEpoch 2/20\n118/118 [==============================] - 132s 1s/step - d_loss: -14.7015 - g_loss: -10.0758\nEpoch 3/20\n118/118 [==============================] - 133s 1s/step - d_loss: -13.3877 - g_loss: -7.3716\nEpoch 4/20\n118/118 [==============================] - 134s 1s/step - d_loss: -12.3851 - g_loss: -3.8300\nEpoch 5/20\n118/118 [==============================] - 134s 1s/step - d_loss: -11.3048 - g_loss: -0.9879\nEpoch 6/20\n118/118 [==============================] - 133s 1s/step - d_loss: -10.3099 - g_loss: 2.5468\nEpoch 7/20\n118/118 [==============================] - 134s 1s/step - d_loss: -9.5120 - g_loss: 5.9738\nEpoch 8/20\n118/118 [==============================] - 134s 1s/step - d_loss: -8.8022 - g_loss: 7.2490\nEpoch 9/20\n118/118 [==============================] - 134s 1s/step - d_loss: -8.1207 - g_loss: 8.1141\nEpoch 10/20\n118/118 [==============================] - 134s 1s/step - d_loss: -7.4981 - g_loss: 11.2222\nEpoch 11/20\n118/118 [==============================] - 134s 1s/step - d_loss: -6.9523 - g_loss: 11.8143\nEpoch 12/20\n118/118 [==============================] - 134s 1s/step - d_loss: -6.4738 - g_loss: 11.9340\nEpoch 13/20\n118/118 [==============================] - 133s 1s/step - d_loss: -6.0315 - g_loss: 11.0130\nEpoch 14/20\n118/118 [==============================] - 134s 1s/step - d_loss: -5.5923 - g_loss: 12.5752\nEpoch 15/20\n118/118 [==============================] - 134s 1s/step - d_loss: -5.1941 - g_loss: 13.1351\nEpoch 16/20\n118/118 [==============================] - 134s 1s/step - d_loss: -4.7675 - g_loss: 11.9371\nEpoch 17/20\n118/118 [==============================] - 134s 1s/step - d_loss: -4.5313 - g_loss: 11.3642\nEpoch 18/20\n118/118 [==============================] - 134s 1s/step - d_loss: -4.2195 - g_loss: 10.0321\nEpoch 19/20\n118/118 [==============================] - 134s 1s/step - d_loss: -3.9090 - g_loss: 11.2887\nEpoch 20/20\n118/118 [==============================] - 134s 1s/step - d_loss: -3.6384 - g_loss: 8.9992\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f92fbda5c10>"},"metadata":{}}]},{"cell_type":"markdown","source":"Display the last generated images:","metadata":{"id":"4RHxCu-2viuT"}},{"cell_type":"code","source":"from IPython.display import Image, display\n\ndisplay(Image(\"generated_img_0_19.png\"))\ndisplay(Image(\"generated_img_1_19.png\"))\ndisplay(Image(\"generated_img_2_19.png\"))","metadata":{"id":"dDQ8hFUgviuT","outputId":"eed4f41c-34d2-495c-ec52-d75530b68e36","colab":{"base_uri":"https://localhost:8080/","height":101},"execution":{"iopub.status.busy":"2022-12-17T09:19:15.383812Z","iopub.execute_input":"2022-12-17T09:19:15.384323Z","iopub.status.idle":"2022-12-17T09:19:15.399433Z","shell.execute_reply.started":"2022-12-17T09:19:15.384265Z","shell.execute_reply":"2022-12-17T09:19:15.397987Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACZUlEQVR4nAXBPYtdVRQG4PWuvc/H3udr7oSrwyASMEjSqGjAQgslNhZDCjsLrcSPxlYQ/4G92AoGxELLWAiiRDCdYGOTQQ2SMIrj3Iy59+yz1uvzYFUUoJgQFAPhBHOIZnHWxcVLmd1E6EqnepHzjfj8ACpwmgvpFFdUVXGIlX9NBOruiwgpal6l0TYLDl46gBXDLH1OuU0p5y5NUxNEHru5sd07uUr9IFNqu5xzzk1eV4hPn5iZ+9lRl8c+ajAHRKzSf3z146XA+5/3T974lSSkG3LOuaubcYjXTvz80+dUpGq6No97Mg4pp5TqOlbPbNw+zDWAsHpkL+Uc6UJxiSZPfZt+f+uHkmuDFRN1RChFKs7yytfVH1f/1lDUVYNQhRjh5urzdNyfXPpPY37Q7BpAZ7GoJERM198Pm2fPdd2yRuCuGEQYSaU4rl/m6/dCN8y2f3jh9nbxNENVxUUwfMCPv0HSv84390/l7UOwCCCPjjmn7oWfP2tDaKc8qSpu/5JD36dRiyuJ/Mn7S9XULEtzGHl2sULlYAwuwerHZXGnBdnpmYTLZ7u4U6G6wNm++PJaXMVAedje1Ddco1PU6bTl4Nq7Kg2ai4Hje1ff/C5WLqRMuc1t+/yfd/ebIbRjfeHLO0dN03Z9yoOqAuRv24MvroD6xK3jV+/e0gZGQmTVpb5puo92ZXt673Te/nR9laeua3POKbq6R/pX69f2+77cOTqOFRaqqlAwLUJAQa+HU/WHbMMWSohQsUcDSJqYQBDUSQBUCxq1UNwFQaOg0JagrmCgIkRATUNwh6kDhKKCqZKB/wPu9jXkXwEA7wAAAABJRU5ErkJggg==\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACbElEQVR4nAXBO4hdVRQG4H+tvfbZ+9zHMIRknEIcNCiCjYKSwggDKUQCKvgAm6mVVGIl2tgFrSwsLKbU2kIEwaCTxgQEMSSDCRiJJkqYkItzZ+4995y91+/3ycmlCaXAxV3UnM7gojUJo1X2AlahQJxVQyWlCrsogxUKnAJAqKQTGggEJ4uKCCEiAq/KysGHlRIOFrUqTrp6ALLn8TOzm8eEAiB7C8UhJhIGMb77gSyv//bNtaIKABinlFLKKSfL2w+9DF3X7aQmpza3GOecmtE0hnhqd+XutS9eztp03LZRAUDRF578cCcKuP/JtcI3bFFAwzSP2pzUtm6s3P349Rw2v/znzpaklCeY5PGoTfHUxdmyW/z4uFq0J/7oP7Um5SnW8yjlmN/5+ffdl55diyGqTnZml9ctt41VuFAnm/e+v/pLR6ib9+08nblcpBoRHN7fWvvpoUVQZWM2vzRvgrs1BgEpZXjz3PpXX6wks4rc+WvzdEJIJqgFYevjp4J89GCvLB4sC3Vjm3q7hzlVnW+dMZ/vv/xa+ezAWHLU+f6sugEUyJOss7f/ff/56Xvrj567/9zyaG/hLga41/SidX/eW92evvDKq5wwCM9vfn1/YUVVJVx/hO2FcvbprJwdjDd4uPf53CEjUIWPXdqAECjdr7s/oD3oqmiEhULX4e6V7To5PPrv2+/+Pjx2CJJAISdWFXC08cjWmrW788a9KBGLqMqJOlQKqoAx6KCo4lBQGNxYKQBV1VkrCwgqSAiCucIpLqUqlQpBDYRQHDB3BwgPVKoLBgtw0jWIWlMrqd7UXsE0OUZTBUpEZfM/M0xaqMCQlDAAAAAASUVORK5CYII=\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACKklEQVR4nCXMO49OURQG4Pdde+1zzneMGYPEhAkSiUgmEZVEJSGh1KoVSr9AQa1Q+A+i0ShFSJQuETqFwj3mZr5MZr7L2Xu9Cs8PeLgylMoQAANTyAoYCSKiepkEETIxJBOtWKiQhhpuEAKKVEAGa6EASBDMh6IMQKLVMCWERMkFyudkRAAGRTBZW+YCFYBoJoZIQgPIVPsrpywTFAA3iCZTtO1qv/lzOHF3fHPHJJJA3/TeNn7w6NqT6ezhUndz/88h5uRN23UuM5NpNi+rjiPu1xubgiQQjqWuG436PvvoxvtH5xbWvpWPTWpz03bdyGdBoEZKzfvbf9fz6mZ/f2gCkAgnECxs7XRMcrLiPLlkewEY4B4Cefzxmm3vfH35+0LfXH62a0gCqoNAjZOXDIs6f/VzpI23W/sWBsHcKIGrpEB2ZxY0vvzmw3QWBgCH2q7t0rFX09ne9qfnP6JOdr48ONs0B5q2w2LTjTpPB48tt03untYo89nuPc/tqO8NUCCl6fZeRZne2VKZjH/NlpuoVSYKAGDuOfn8W9SNW9fencuKkFcQQascULV4cYXx4t3uPOdcQQcYkFBFabY1YrweD+spU6wuSJQAQArOY/KjEJVGyEyQANII03ijli3QkomA5wijAMJC2HubvhsgVIDB5an0v04S7PDA+b4JIgSPgCiCUhiGbaTMmhgEZSmZQIAEYckUgygpQvCpGRkMr8mCVkOhlJSqqfwDIeRGt3Pwi78AAAAASUVORK5CYII=\n","text/plain":"<IPython.core.display.Image object>"},"metadata":{}}]}]}